#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass ulb_thesis
\options draftfoot
\use_default_options false
\maintain_unincluded_children false
\language american
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 0
\use_package mathdots 0
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title

\lang english
Engineering a Runtime System for AQL
\end_layout

\begin_layout Author
your name
\end_layout

\begin_layout Degree
Docteur en XXX
\begin_inset Newline newline
\end_inset

(Département XXX, Faculté XXX)
\end_layout

\begin_layout Dedication
To my parents.
\end_layout

\begin_layout Acknowledgments
Thank you, my advisor!
\end_layout

\begin_layout Standard
\begin_inset VSpace 4cm*
\end_inset


\end_layout

\begin_layout Standard

\emph on
\begin_inset Quotes eld
\end_inset

A great citation here
\emph default

\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
\align right
X
\end_layout

\begin_layout Abstract
abstract of the thesis
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList figure

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList table

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
mainmatter
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset include
LatexCommand include
filename "intro.lyx"

\end_inset


\end_layout

\begin_layout Chapter
Information Extraction
\end_layout

\begin_layout Section
Definition
\end_layout

\begin_layout Standard
Information Extraction (IE) is the discipline that addresses the task of
 extracting structured information from unstructured sources automatically.
 Sources usually take the form of text documents.
 The most explored aspect of IE is the extraction of 
\emph on
named entities.

\emph default
 Another main topic, that has become object of research only in recent years,
 is the extraction of 
\emph on
relationships
\emph default
 between entities.
 IE is a field with contributions from many different communities: Machine
 Learning, Information Retrieval, Database, Web, Document Analysis.
 The interest in towards it is motivated by the evergrowing amount of data
 that is generated by our society, that is at a point in which manual analysis
 is infeasible.
 Thus IE promises to bring value to many application domains, most notably
 the enterprise world and the Web.
 There are two main approaches to IE: 
\emph on
rule-based 
\emph default
and 
\emph on
statistical
\emph default
.
 
\end_layout

\begin_layout Section
Example Applications
\end_layout

\begin_layout Paragraph
News Tracking
\end_layout

\begin_layout Standard
The activity of tracking events in news articles.
 It can result in a lot of useful services, like automatic creation of multimedi
a news, linking articles to information pages on the entities found, etc.
\end_layout

\begin_layout Paragraph
Data Cleaning
\end_layout

\begin_layout Standard
Extracting structured forms from flat data strings (containing, e.g., addresses).
 It allows more effective decuplication of information, among the other
 things.
\end_layout

\begin_layout Paragraph
Citation Databases
\end_layout

\begin_layout Standard
Articles, conferences sites, individual research sites and similar are explored
 to obain formatted citations of publications, later stored in publicly
 accessible databases.
 They are capable to forward references and may provide aggregate statistics
 and scoring information.
 See for instance Google Scholar (
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://scholar.google.it/
\end_layout

\end_inset

).
\end_layout

\begin_layout Paragraph
Relationship Web Search
\end_layout

\begin_layout Standard
Relationship extraction would be a very useful feature to integrate into
 web search engines, as the keyword search that they offer at present time
 is only good for entity identification.
\end_layout

\begin_layout Section
Issues
\end_layout

\begin_layout Subsection
Accuracy
\end_layout

\begin_layout Standard
We measure the accuracy of an IE task with two quantities:
\end_layout

\begin_layout Itemize

\emph on
precision
\emph default
: the percentage of correctly extracted entities among the extracted entities;
\end_layout

\begin_layout Itemize

\emph on
recall
\emph default
: the percentage of entities extracted among all the existing entities in
 the input source.
\end_layout

\begin_layout Standard
The main difficulties to achieving a good level of accuracy are:
\end_layout

\begin_layout Itemize
the availability of a great set of clues that might be very different in
 nature (e.
 g.
 orghographic properties, part of speech, typical words, etc.) and that might
 be difficult to combine;
\end_layout

\begin_layout Itemize
the difficulty of finding out missed extractions;
\end_layout

\begin_layout Itemize
the fact that with the advancement of research the extracted data structures
 keep increasing in complexity (for instance it is becoming more difficult
 to identify the boundaries of an entity in a text).
\end_layout

\begin_layout Standard
While we are able to reach a very good level of accuracy for entity extraction
 (
\begin_inset Formula $90\%$
\end_inset

), relationship extraction is still quite unreliable (
\begin_inset Formula $50-70\%$
\end_inset

 accuracy), mainly due to its intrinsic complexity, superior to that of
 the task of entity extraction.
\end_layout

\begin_layout Subsection
Running Time
\end_layout

\begin_layout Standard
In its initial phase IE was interesing only to research communities.
 With the enormous amount of data they need to process, companies are now
 concerned with IE too, and 
\emph on
scalability
\emph default
 has become a central issue, while many IE systems don't address the problem
 of efficiently carrying out extraction tasks with sufficient attention.
 Efficiency is influenced most notably by the efficiency of the following
 tasks:
\end_layout

\begin_layout Itemize
filtering documents in order to actually examine only the ones that have
 good chances to contain the desired information;
\end_layout

\begin_layout Itemize
focusing on the parts of a document that contain relevant information;
\end_layout

\begin_layout Itemize
processing steps, like database querying, that are typically very expensive
 and that might have to be performed on the selected pieces of input.
\end_layout

\begin_layout Standard
Recently many solutions have been proposed to target the scalability issue;
 one of these is indeed SystemT.
\end_layout

\begin_layout Section
Entity Extraction
\end_layout

\begin_layout Standard
Named entities are elements of interest in a text.
 Example of entities are person names, street addresses, institutions, countries
, and so on.
 In this section and in the next one, I assume that the output of an extraction
 task is a series of labels inserted into an input text document, although
 there are other possibilities.
\end_layout

\begin_layout Subsection
Rule-based Methods
\end_layout

\begin_layout Standard
Rule-based methods employ sets of predicates that are 
\begin_inset Quotes eld
\end_inset

fired
\begin_inset Quotes erd
\end_inset

 independently.
 When a portion of input text satisfies a predicate, the predicate's associated
 action is executed.
 A predicate can be represented in the following generic form:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\textrm{"Contextual Pattern}\longrightarrow\textrm{Action"}
\end{equation}

\end_inset


\begin_inset Newline newline
\end_inset

Contextual patterns are a way to identify entitities by exploiting their
 properties or the context in which they are usually met.
 The most common formalism used to express them is that of 
\emph on
regular expressions
\emph default
 over tokens of the input.
 Actions mark the entities that have been identified, and might consist
 in storing them in a database or adding delimiters directly in the text.
\begin_inset Newline newline
\end_inset

Most systems in this category present a cascaded structure: an input document
 goes through a series of processing phases, and the output of a phase is
 the input of the next one.
 A famous example is that of cascading grammars: formal grammars are evaluated
 in sequence on the input.
\end_layout

\begin_layout Standard
As I said, a contextual pattern seeks for (groups of) tokens that have certain
 features.
 In the case of entity extraction, features can be classified in the following
 categories:
\end_layout

\begin_layout Itemize
string representation;
\end_layout

\begin_layout Itemize
orthography (e.g.
 small case, mixed case, number, etc.);
\end_layout

\begin_layout Itemize
part of speech;
\end_layout

\begin_layout Itemize
list of dictionaries they're contained into;
\end_layout

\begin_layout Itemize
annotation obtained in previous phases.
\end_layout

\begin_layout Standard
Rules can be hand-coded by experts or learned through learning algorithms.
 In the second case the goal is to cover each of the entities of interest
 in the training set with at least one rule.
 The obtained rules should have good recall and precision on new input.
 Moreover, when learning a set of rules, we would like to achieve a good
 level of 
\emph on
generalizability
\emph default
, that is: we would like to find the minimum set of rules accounting for
 the maximum portion of training data, with high precision.
 There are two main strategies for rule learning: 
\emph on
bottom-up
\emph default
 (start from very specific rules and make them more and more general) and
 
\emph on
top-down
\emph default
 (start with rules covering all existing instances, then specialize them).
\end_layout

\begin_layout Subsubsection
Example Rules
\end_layout

\begin_layout Standard
For the examples I use an idealized syntax, as in REF.
\end_layout

\begin_layout Example
Consider the task of identifying all mentions ISO standards in a text.
 A rule for this purpose could be:
\end_layout

\begin_layout Example
\begin_inset Formula 
\begin{multline}
\left(\left\{ \textrm{String}=\textrm{"ISO"}\right\} \left\{ \textrm{String}=\textrm{"/IEC"}\right\} \left\{ \textrm{?}\right\} \left\{ \textrm{String}=\textrm{"/ASTM"}\right\} \left\{ \textrm{?}\right\} \right.\\
\left.\left\{ \textrm{Orthography type}=\textrm{number}\right\} \left\{ \left\{ \textrm{String}=\textrm{":"}\right\} \left\{ \textrm{Orthography type}=\textrm{number}\right\} \right\} \left\{ \textrm{?}\right\} \right)\\
\longrightarrow\textrm{ISO Standards}
\end{multline}

\end_inset


\end_layout

\begin_layout Standard
a
\end_layout

\begin_layout Example
Multiple entities can be matched at once.
 Imagine we need to find mentions of (simple) street addresses in a text
 consisting of a street name and a street number.
 A rule that matches the name and number separatedly could be:
\end_layout

\begin_layout Example
\begin_inset Formula 
\begin{multline}
\left(\left\{ \textrm{Orthography type}=\textrm{mixed case word}\right\} \left\{ \textrm{*}\right\} \right)\textrm{: Name}\left(\left\{ \textrm{String}=\textrm{","}\right\} \right)\\
\left(\left\{ \textrm{Orthography type}=\textrm{number}\right\} \right)\textrm{: Number}\\
\longrightarrow\textrm{Street Name}=:\textrm{Name, }\textrm{Street Number}=:\textrm{Number}
\end{multline}

\end_inset


\end_layout

\begin_layout Subsection
Statistical Methods
\end_layout

\begin_layout Standard
Statistical methods aim to decompose the source, assigning a label to each
 element in the decomposition.
 We distinguish between three types of statistical models:
\end_layout

\begin_layout Itemize

\emph on
Token-level models
\emph default
: they assign a label to each token of the source.
 Since entities are usually comprised of multiple adjacent tokens, the tags
 used are of the forms 
\begin_inset Quotes eld
\end_inset

entity_begin
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

entity_continue
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

entity_end
\begin_inset Quotes erd
\end_inset

;
\end_layout

\begin_layout Itemize

\emph on
Segment-level models
\emph default
: they try to find the best segmentation of the source text;
\end_layout

\begin_layout Itemize

\emph on
Grammar-based models: 
\emph default
they use formal grammars, outputting parse trees.
 All the valid parses are considered for an input document, assigning a
 score to each.
 The parse with the highest score is retained.
\end_layout

\begin_layout Standard
I now give a brief description of Token-level Models and Segment-level Models.
\end_layout

\begin_layout Subsubsection
Token-Level Models
\begin_inset CommandInset label
LatexCommand label
name "sub:Token-Level-Models"

\end_inset


\end_layout

\begin_layout Paragraph
Features
\end_layout

\begin_layout Standard
Clues are of the form 
\begin_inset Formula 
\begin{equation}
f:\left(x,y,i\right)\longmapsto\mathbb{R}
\end{equation}

\end_inset

where 
\begin_inset Formula $x$
\end_inset

 is a sequence of tokens, 
\begin_inset Formula $i$
\end_inset

 is a position in 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

 is a candidate label for the token at 
\begin_inset Formula $i$
\end_inset

.
 We distinguish between these types of features:
\end_layout

\begin_layout Itemize

\emph on
word features
\emph default
;
\end_layout

\begin_layout Itemize

\emph on
orthographic features;
\end_layout

\begin_layout Itemize

\emph on
dictionary lookup features.
\end_layout

\begin_layout Paragraph
Models for Labeling Token
\end_layout

\begin_layout Standard
The best models are the ones that take into account dependencies between
 tokens, among which we may find:
\end_layout

\begin_layout Itemize

\emph on
ordered classifiers
\emph default
;
\end_layout

\begin_layout Itemize

\emph on
Hidden Markov models;
\end_layout

\begin_layout Itemize

\emph on
Maximum Entropy Taggers;
\end_layout

\begin_layout Itemize

\emph on
Conditional Markov Models;
\end_layout

\begin_layout Itemize

\emph on
Conditional Random Fields 
\emph default
(the state of the art).
\end_layout

\begin_layout Subsubsection
Segment-Level Models
\end_layout

\begin_layout Paragraph
Features
\end_layout

\begin_layout Standard
In these models the label for a segment depends on the properties of its
 token and on the previous segment.
 We can describe a segment 
\begin_inset Formula $s_{j}$
\end_inset

 as:
\begin_inset Formula 
\begin{equation}
s_{j}=\left(y_{i},l_{j},u_{j}\right)\label{eq:segment}
\end{equation}

\end_inset

where 
\begin_inset Formula $y_{i}$
\end_inset

 is the proposed label for 
\begin_inset Formula $s_{j}$
\end_inset

 and 
\begin_inset Formula $l_{j},u_{j}$
\end_inset

 are the start and end positions of 
\begin_inset Formula $s_{j}$
\end_inset

.
 Therefore, a feature is of the form:
\begin_inset Formula 
\begin{equation}
f\left(y_{i},y_{i-1},x,l_{j},u_{j}\right)
\end{equation}

\end_inset

where 
\begin_inset Formula $x$
\end_inset

 is the input sequence of tokens, 
\begin_inset Formula $y_{i-1}$
\end_inset

 is the proposed label for the previous segment and the other symbols are
 defined as for Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:segment"

\end_inset

.
 Besides token-level features (see subsection 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Token-Level-Models"

\end_inset

), we can exploit the following feature types:
\end_layout

\begin_layout Itemize

\emph on
Similarity to an entity in a database;
\end_layout

\begin_layout Itemize

\emph on
Length of the segment.
\end_layout

\begin_layout Standard
There are also global segmentation models, that try to find the best segmentatio
n of a token sequence by maximizing a target function.
\end_layout

\begin_layout Section
Relationship Extraction
\end_layout

\begin_layout Standard
When exctracting relationship between entities, we might face three types
 of specific tasks:
\end_layout

\begin_layout Itemize
given a pair of entities, find the relationship between them;
\end_layout

\begin_layout Itemize
given an entity 
\begin_inset Formula $e$
\end_inset

 and a relationship 
\begin_inset Formula $r$
\end_inset

, find all the other entities 
\begin_inset Formula $e'$
\end_inset

 such that 
\begin_inset Formula $\left(e,e'\right)\in r$
\end_inset

;
\end_layout

\begin_layout Itemize
given a big and open-ended input and a relationship 
\begin_inset Formula $r$
\end_inset

 find all pairs of entities 
\begin_inset Formula $e',e''$
\end_inset

 such that 
\begin_inset Formula $\left(e',e''\right)\in r$
\end_inset

.
\end_layout

\begin_layout Subsection
Predicting the Relationship between a Entity Pair 
\end_layout

\begin_layout Standard
For the first task, we can exploit the following resources:
\end_layout

\begin_layout Itemize

\emph on
surface tokens
\emph default
: tokens that are usually placed between entities.
 They are strong clues;
\end_layout

\begin_layout Itemize

\emph on
part of speech tags 
\emph default
(the most important being verbs);
\end_layout

\begin_layout Itemize

\emph on
syntactic parse tree
\emph default
: allows grouping words in phrase types, e.g.
 noun phrases, propositional phrases, and so on;
\end_layout

\begin_layout Itemize

\emph on
dependency graph
\emph default
: it is a less expensive structure to compute than the parse tree and it
 links a word to those that depend on it.
\end_layout

\begin_layout Standard
The main methods available to carry out the task are:
\end_layout

\begin_layout Itemize

\emph on
Feature-based methods
\emph default
, that simply transform the clues mentioned above for usage by conventional
 classifier models;
\end_layout

\begin_layout Itemize

\emph on
Kernel-based methods
\emph default
, that use kernel functions to encode the similarity between two graphs;
\end_layout

\begin_layout Itemize

\emph on
Rule-based methods
\emph default
, creating rules over structures around pairs of entities.
\end_layout

\begin_layout Standard
The second task is a special case of the third one so I don't treat it directly.
\end_layout

\begin_layout Subsection
Finding All Possible Entity Pairs Belonging to a Relationship
\end_layout

\begin_layout Standard
The third task can be met especially when dealing with the Web.
 Usually we can exploit the following resources to fulfill it:
\end_layout

\begin_layout Itemize
the 
\emph on
types of arguments
\emph default
 of 
\begin_inset Formula $r$
\end_inset

 (that might need specific recognition patterns);
\end_layout

\begin_layout Itemize
a 
\emph on
seed database
\emph default
 of pairs of entities belonging to 
\begin_inset Formula $r$
\end_inset

;
\end_layout

\begin_layout Itemize

\emph on
manually coded patterns
\emph default
.
\end_layout

\begin_layout Standard
The generic procedure that is used in this case can be described with these
 steps:
\end_layout

\begin_layout Enumerate
Use the seed database to learn the relevant extraction patterns;
\end_layout

\begin_layout Enumerate
Use the obtained patterns to define candidate triples of the form 
\begin_inset Formula $\left(e',e'',r\right)$
\end_inset

;
\end_layout

\begin_layout Enumerate
Retain a subset of the candidate triples, using a statistical test.
\end_layout

\begin_layout Standard
There exist also rule-based methods for the task.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset include
LatexCommand include
filename "conclusion.lyx"

\end_inset


\end_layout

\begin_layout Standard

\lang english
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "xampl"
options "apalike"

\end_inset


\end_layout

\begin_layout Standard
\start_of_appendix
\begin_inset CommandInset include
LatexCommand include
filename "appendix.lyx"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset index_print
LatexCommand printindex
type "idx"

\end_inset


\end_layout

\end_body
\end_document
